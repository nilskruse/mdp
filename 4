use std::collections::{BTreeMap, HashMap};

use rand_chacha::ChaCha20Rng;

use crate::{
    mdp::{self, Action, Mdp, Reward, State},
    policies::epsilon_greedy_policy,
};

use super::StateActionAlgorithm;

pub struct MonteCarlo {
    max_steps: usize,
    epsilon: f64,
}

impl MonteCarlo {
    pub fn new(max_steps: usize, epsilon: f64) -> MonteCarlo {
        MonteCarlo { max_steps, epsilon }
    }

    fn generate_episode(
        &self,
        mdp: &Mdp,
        q_map: &BTreeMap<(State, Action), Reward>,
        rng: &mut ChaCha20Rng,
    ) -> Vec<(State, Action, Reward)> {
        let mut episode = Vec::with_capacity(self.max_steps);

        let mut current_state = mdp.initial_state;
        let mut steps = 0;
        while !mdp.terminal_states.contains(&current_state) && steps < self.max_steps {
            let selected_action =
                epsilon_greedy_policy(mdp, q_map, current_state, self.epsilon, rng);

            if let Some(selected_action) = selected_action {
                let (next_state, reward) =
                    mdp.perform_action((current_state, selected_action), rng);

                episode.push((current_state, selected_action, reward));
                current_state = next_state;
                steps += 1;
            } else {
                break;
            }
        }
        episode
    }
}

impl StateActionAlgorithm for MonteCarlo {
    fn run_with_q_map(
        &self,
        mdp: &crate::mdp::Mdp,
        episodes: usize,
        rng: &mut rand_chacha::ChaCha20Rng,
        q_map: &mut std::collections::BTreeMap<(crate::mdp::State, crate::mdp::Action), f64>,
    ) {

        for _ in 0..episodes {
            // generate episode
            let episode = self.generate_episode(mdp, q_map, rng);
            let g = HashMap::new();
            
            episode.iter().for_each(|(state, action, reward)|{
                g.entry(kk)
            });

        }
    }
}
